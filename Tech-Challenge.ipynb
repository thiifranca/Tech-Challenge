{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0493424e",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 com Dataset Amazon-1.3M\n",
    "\n",
    "**Objetivo:** Treinar GPT-2 para responder perguntas sobre produtos Amazon.\n",
    "\n",
    "**GPU:** Otimizado para NVIDIA RTX 3070 (8GB VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c9c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "üíæ VRAM: 8.0 GB\n",
      "üîß CUDA: 11.8\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Verificar GPU\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"‚ùå GPU n√£o detectada! Este notebook requer GPU.\"\n",
    "\n",
    "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"üîß CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ab0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niloa\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1007 11:44:29.347000 19532 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "W1007 11:44:29.347000 19532 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Bibliotecas carregadas\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Imports\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"üì¶ Bibliotecas carregadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33230f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Carregando dataset...\n",
      "‚úÖ 100000 amostras carregadas\n",
      "\n",
      "üìù Exemplo:\n",
      "Pergunta: O que √© 'Girls Ballet Tutu Neon Pink'? Resposta: High quality 3 layer ballet tutu. 12 inches in length...\n",
      "‚úÖ 100000 amostras carregadas\n",
      "\n",
      "üìù Exemplo:\n",
      "Pergunta: O que √© 'Girls Ballet Tutu Neon Pink'? Resposta: High quality 3 layer ballet tutu. 12 inches in length...\n"
     ]
    }
   ],
   "source": [
    "# 3Ô∏è‚É£ Carregar Dataset Amazon\n",
    "def ler_json(caminho, max_linhas=100000):\n",
    "    linhas = []\n",
    "    with open(caminho, \"r\", encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_linhas:\n",
    "                break\n",
    "            try:\n",
    "                linhas.append(json.loads(line))\n",
    "            except:\n",
    "                continue\n",
    "    return linhas\n",
    "\n",
    "print(\"üì• Carregando dataset...\")\n",
    "raw_data = ler_json(\"LF-Amazon-1.3M/trn.json\", max_linhas=100000)\n",
    "df = pd.DataFrame(raw_data)[[\"title\", \"content\"]].dropna()\n",
    "\n",
    "# Criar formato: Pergunta + Resposta\n",
    "df[\"text\"] = df.apply(lambda x: f\"Pergunta: O que √© '{x['title']}'? Resposta: {x['content']}\", axis=1)\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
    "\n",
    "print(f\"‚úÖ {len(dataset)} amostras carregadas\")\n",
    "print(f\"\\nüìù Exemplo:\\n{dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477c021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:18<00:00, 5313.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokeniza√ß√£o conclu√≠da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4Ô∏è‚É£ Tokeniza√ß√£o\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(\"‚úÖ Tokeniza√ß√£o conclu√≠da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117e50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo GPT-2 carregado (FP16)\n",
      "üíæ Mem√≥ria GPU: 0.24 GB\n"
     ]
    }
   ],
   "source": [
    "# 5Ô∏è‚É£ Carregar Modelo GPT-2\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "memory_gb = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"‚úÖ Modelo GPT-2 carregado (FP16)\")\n",
    "print(f\"üíæ Mem√≥ria GPU: {memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e65c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTE BASELINE (sem fine-tuning)\n",
      "\n",
      "Q: Game of Thrones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niloa\\AppData\\Local\\Temp\\ipykernel_19532\\2884618824.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: La bajo que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Resposta: O que √© 'Game of Thrones'? Respost\n",
      "\n",
      "Q: Smartphone Samsung Galaxy\n",
      "R: Esta, c√≥mo que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n",
      "\n",
      "Casa: Esta, o que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n",
      "\n",
      "Casa: Esta, o que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n",
      "R: Esta, c√≥mo que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n",
      "\n",
      "Casa: Esta, o que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n",
      "\n",
      "Casa: Esta, o que esse cada cada. Dios, o que s√≥lo, e a las comentarios, cada donde, donde.\n"
     ]
    }
   ],
   "source": [
    "# 6Ô∏è‚É£ Teste ANTES do Fine-Tuning\n",
    "def gerar_resposta(pergunta, max_tokens=120):\n",
    "    prompt = f\"Pergunta: O que √© '{pergunta}'? Resposta:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,  # Necess√°rio para usar temperature e top_p\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    texto = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Tentar extrair apenas a resposta, mas mostrar tudo se falhar\n",
    "    if \"Resposta:\" in texto:\n",
    "        partes = texto.split(\"Resposta:\", 1)\n",
    "        if len(partes) > 1:\n",
    "            resposta = partes[1].split(\"Pergunta:\")[0].strip()\n",
    "            if resposta:\n",
    "                return resposta\n",
    "    \n",
    "    # Se n√£o conseguir extrair, retornar o texto completo ap√≥s o prompt\n",
    "    return texto[len(prompt):].strip() if len(texto) > len(prompt) else texto\n",
    "\n",
    "print(\"üß™ TESTE BASELINE (sem fine-tuning)\\n\")\n",
    "print(\"Q: Game of Thrones\")\n",
    "resposta1 = gerar_resposta('Game of Thrones')\n",
    "print(f\"R: {resposta1}\\n\")\n",
    "print(\"Q: Smartphone Samsung Galaxy\")\n",
    "resposta2 = gerar_resposta('Smartphone Samsung Galaxy')\n",
    "print(f\"R: {resposta2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7fa6ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niloa\\AppData\\Local\\Temp\\ipykernel_19532\\3694051161.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando treinamento...\n",
      "üìä Batch efetivo: 50 = 50\n",
      "üìä Total de amostras: 100000\n",
      "üìä Total steps: ~2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 1:07:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.133600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Treinamento conclu√≠do!\n",
      "üíæ Mem√≥ria GPU usada: 1.43 GB\n"
     ]
    }
   ],
   "source": [
    "# 7Ô∏è‚É£ Fine-Tuning\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Recarregar modelo em FP32 (mais compat√≠vel)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=10,   # Conservador e seguro\n",
    "    gradient_accumulation_steps=5,    # Batch efetivo: 50\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,         # Sem workers (mais est√°vel)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"üöÄ Iniciando treinamento...\")\n",
    "print(f\"üìä Batch efetivo: {10 * 5} = 50\")\n",
    "print(f\"üìä Total de amostras: {len(tokenized_dataset)}\")\n",
    "print(f\"üìä Total steps: ~{len(tokenized_dataset) // 50}\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
    "print(f\"üíæ Mem√≥ria GPU usada: {memory_used:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88be3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loss: 3.0813\n",
      "üìä Perplexity: 21.79\n"
     ]
    }
   ],
   "source": [
    "# 8Ô∏è‚É£ Avaliar Modelo\n",
    "eval_dataset = tokenized_dataset.select(range(1000))\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "eval_loss = metrics.get(\"eval_loss\")\n",
    "perplexity = math.exp(eval_loss) if eval_loss else None\n",
    "\n",
    "print(f\"üìä Loss: {eval_loss:.4f}\")\n",
    "print(f\"üìä Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af61d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TESTE P√ìS FINE-TUNING\n",
      "\n",
      "Q: Game of Thrones\n",
      "R: \"I would say that this is the best introduction to Game's characters, from start-to&#x2019;end. It tells a story with believable plot and compelling character development.\"--Publishers Weekly\"An excellent selection...the book will be enjoyed by all fans who love fantasy novels but are looking for an engaging read....This one should appeal equally well at home or abroad as it does in my native language,\" said author Joffrey Baratheon \". . .\" --Library Journal on The Book Of Snow WhiteandThe Night Before Christmas\"...A good reading choice if you're into action series!\" -Tales From A Memory (Penguin Classics)On This Day And On That OtherDayAnd I Can Read You Now:\"\n",
      "\n",
      "Q: Smartphone Samsung Galaxy\n",
      "R: \"I would say that this is the best introduction to Game's characters, from start-to&#x2019;end. It tells a story with believable plot and compelling character development.\"--Publishers Weekly\"An excellent selection...the book will be enjoyed by all fans who love fantasy novels but are looking for an engaging read....This one should appeal equally well at home or abroad as it does in my native language,\" said author Joffrey Baratheon \". . .\" --Library Journal on The Book Of Snow WhiteandThe Night Before Christmas\"...A good reading choice if you're into action series!\" -Tales From A Memory (Penguin Classics)On This Day And On That OtherDayAnd I Can Read You Now:\"\n",
      "\n",
      "Q: Smartphone Samsung Galaxy\n",
      "R: An excellent introduction to the world of mobile phone technology. This book is full-text, easy to read and covers all aspects of smart phones in a clear manner that should be familiar to anyone with an interest or even just someone who has recently switched from Android devices...This new edition also includes some important information about how smartphones are manufactured for different markets--including features such as charging circuitry (if you don't have one already), screen size, battery life, camera resolution, etc.--and explains why it's possible to use them together so effectively without sacrificing functionality by using separate components/dishwashers like capacitors(which can add up quickly).The text on this page assumes no prior knowledge of software development techniques&mdash;such as\n",
      "R: An excellent introduction to the world of mobile phone technology. This book is full-text, easy to read and covers all aspects of smart phones in a clear manner that should be familiar to anyone with an interest or even just someone who has recently switched from Android devices...This new edition also includes some important information about how smartphones are manufactured for different markets--including features such as charging circuitry (if you don't have one already), screen size, battery life, camera resolution, etc.--and explains why it's possible to use them together so effectively without sacrificing functionality by using separate components/dishwashers like capacitors(which can add up quickly).The text on this page assumes no prior knowledge of software development techniques&mdash;such as\n"
     ]
    }
   ],
   "source": [
    "# 9Ô∏è‚É£ Teste DEPOIS do Fine-Tuning\n",
    "def gerar_resposta_finetuned(pergunta, max_tokens=150):\n",
    "    \"\"\"Fun√ß√£o simplificada para gerar respostas ap√≥s o fine-tuning\"\"\"\n",
    "    prompt = f\"Pergunta: O que √© '{pergunta}'? Resposta:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    \n",
    "    texto = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    resposta = texto[len(prompt):].strip() if len(texto) > len(prompt) else texto\n",
    "    \n",
    "    return resposta if resposta else \"[Sem resposta]\"\n",
    "\n",
    "print(\"üéØ TESTE P√ìS FINE-TUNING\\n\")\n",
    "\n",
    "print(\"Q: Game of Thrones\")\n",
    "resposta1 = gerar_resposta_finetuned('Game of Thrones')\n",
    "print(f\"R: {resposta1}\\n\")\n",
    "print(\"Q: Smartphone Samsung Galaxy\")\n",
    "resposta2 = gerar_resposta_finetuned('Smartphone Samsung Galaxy')\n",
    "print(f\"R: {resposta2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
